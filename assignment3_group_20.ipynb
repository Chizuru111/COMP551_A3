{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "yu_5hJNc5-wX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from IPython.core.debugger import set_trace\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from typing import List\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hRNsvGht1F8"
   },
   "source": [
    "# New section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "VGzUdHG46vwK"
   },
   "outputs": [],
   "source": [
    "# for dirname, _, filenames in os.walk(('/kaggle/input')):\n",
    "#   for filename in filenames:\n",
    "#     print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBWyCCghKUwi"
   },
   "source": [
    "# Loading and Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "8iwkVzNhKkhy"
   },
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('~/Programming/COMP551/COMP551_A3/dataset/sign_mnist_train.csv')\n",
    "test_df=pd.read_csv('~/Programming/COMP551/COMP551_A3/dataset/sign_mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "j41KLHiYNKYb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "      <td>27455.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.318813</td>\n",
       "      <td>145.419377</td>\n",
       "      <td>148.500273</td>\n",
       "      <td>151.247714</td>\n",
       "      <td>153.546531</td>\n",
       "      <td>156.210891</td>\n",
       "      <td>158.411255</td>\n",
       "      <td>160.472154</td>\n",
       "      <td>162.339683</td>\n",
       "      <td>163.954799</td>\n",
       "      <td>...</td>\n",
       "      <td>141.104863</td>\n",
       "      <td>147.495611</td>\n",
       "      <td>153.325806</td>\n",
       "      <td>159.125332</td>\n",
       "      <td>161.969259</td>\n",
       "      <td>162.736696</td>\n",
       "      <td>162.906137</td>\n",
       "      <td>161.966454</td>\n",
       "      <td>161.137898</td>\n",
       "      <td>159.824731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.287552</td>\n",
       "      <td>41.358555</td>\n",
       "      <td>39.942152</td>\n",
       "      <td>39.056286</td>\n",
       "      <td>38.595247</td>\n",
       "      <td>37.111165</td>\n",
       "      <td>36.125579</td>\n",
       "      <td>35.016392</td>\n",
       "      <td>33.661998</td>\n",
       "      <td>32.651607</td>\n",
       "      <td>...</td>\n",
       "      <td>63.751194</td>\n",
       "      <td>65.512894</td>\n",
       "      <td>64.427412</td>\n",
       "      <td>63.708507</td>\n",
       "      <td>63.738316</td>\n",
       "      <td>63.444008</td>\n",
       "      <td>63.509210</td>\n",
       "      <td>63.298721</td>\n",
       "      <td>63.610415</td>\n",
       "      <td>64.396846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>125.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>182.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>204.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              label        pixel1        pixel2        pixel3        pixel4  \\\n",
       "count  27455.000000  27455.000000  27455.000000  27455.000000  27455.000000   \n",
       "mean      12.318813    145.419377    148.500273    151.247714    153.546531   \n",
       "std        7.287552     41.358555     39.942152     39.056286     38.595247   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        6.000000    121.000000    126.000000    130.000000    133.000000   \n",
       "50%       13.000000    150.000000    153.000000    156.000000    158.000000   \n",
       "75%       19.000000    174.000000    176.000000    178.000000    179.000000   \n",
       "max       24.000000    255.000000    255.000000    255.000000    255.000000   \n",
       "\n",
       "             pixel5        pixel6        pixel7        pixel8        pixel9  \\\n",
       "count  27455.000000  27455.000000  27455.000000  27455.000000  27455.000000   \n",
       "mean     156.210891    158.411255    160.472154    162.339683    163.954799   \n",
       "std       37.111165     36.125579     35.016392     33.661998     32.651607   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%      137.000000    140.000000    142.000000    144.000000    146.000000   \n",
       "50%      160.000000    162.000000    164.000000    165.000000    166.000000   \n",
       "75%      181.000000    182.000000    183.000000    184.000000    185.000000   \n",
       "max      255.000000    255.000000    255.000000    255.000000    255.000000   \n",
       "\n",
       "       ...      pixel775      pixel776      pixel777      pixel778  \\\n",
       "count  ...  27455.000000  27455.000000  27455.000000  27455.000000   \n",
       "mean   ...    141.104863    147.495611    153.325806    159.125332   \n",
       "std    ...     63.751194     65.512894     64.427412     63.708507   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...     92.000000     96.000000    103.000000    112.000000   \n",
       "50%    ...    144.000000    162.000000    172.000000    180.000000   \n",
       "75%    ...    196.000000    202.000000    205.000000    207.000000   \n",
       "max    ...    255.000000    255.000000    255.000000    255.000000   \n",
       "\n",
       "           pixel779      pixel780      pixel781      pixel782      pixel783  \\\n",
       "count  27455.000000  27455.000000  27455.000000  27455.000000  27455.000000   \n",
       "mean     161.969259    162.736696    162.906137    161.966454    161.137898   \n",
       "std       63.738316     63.444008     63.509210     63.298721     63.610415   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%      120.000000    125.000000    128.000000    128.000000    128.000000   \n",
       "50%      183.000000    184.000000    184.000000    182.000000    182.000000   \n",
       "75%      208.000000    207.000000    207.000000    206.000000    204.000000   \n",
       "max      255.000000    255.000000    255.000000    255.000000    255.000000   \n",
       "\n",
       "           pixel784  \n",
       "count  27455.000000  \n",
       "mean     159.824731  \n",
       "std       64.396846  \n",
       "min        0.000000  \n",
       "25%      125.500000  \n",
       "50%      182.000000  \n",
       "75%      204.000000  \n",
       "max      255.000000  \n",
       "\n",
       "[8 rows x 785 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "WOvoUakdNT9F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27455 entries, 0 to 27454\n",
      "Columns: 785 entries, label to pixel784\n",
      "dtypes: int64(785)\n",
      "memory usage: 164.4 MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "jiSdvx0eNvv-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7172 entries, 0 to 7171\n",
      "Columns: 785 entries, label to pixel784\n",
      "dtypes: int64(785)\n",
      "memory usage: 43.0 MB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qI78tJABN4Wg"
   },
   "source": [
    "The train_df dataset consit of 1st column representing labels 1 to 24. The label is loaded in a separate dataframe called 'train_label' and the 'label' column is dropped from the original training dataframe which now consist of only 784 pixel values for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "rY3WNnVDOVcX"
   },
   "outputs": [],
   "source": [
    "# Drop the label column for the train_df\n",
    "train_label = train_df['label']\n",
    "trainset = train_df.drop(['label'], axis=1)\n",
    "# Convert the dataframe to numpy array\n",
    "X_train = trainset.values.astype(np.float64)\n",
    "\n",
    "# Same thing for the test_df\n",
    "test_label = test_df['label']\n",
    "testset = test_df.drop(['label'], axis=1)\n",
    "# Convert the dataframe to numpy array\n",
    "X_test = testset.values.astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfpFkDs0RpRv"
   },
   "source": [
    "One-hot encodding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "7AgJuz_GQU6w"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(train_label)\n",
    "y_test = lb.fit_transform(test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K25seuBhSZpE"
   },
   "source": [
    "# Normalizatioin and Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "sJGqaUkTTpKv"
   },
   "outputs": [],
   "source": [
    "X_train_mean = np.mean(X_train)\n",
    "X_train_std = np.std(X_train, axis=0)\n",
    "# For images, subtract a single data from all pixels\n",
    "X_train -= X_train_mean\n",
    "X_train /= X_train_std + 1e-5\n",
    "\n",
    "X_test -= X_train_mean\n",
    "X_test /= X_train_std + 1e-5\n",
    "\n",
    "# Vectorization\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dvcmZuzWfCQ"
   },
   "source": [
    "2 hidden layers MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "QD2SN3m4Wq0d"
   },
   "outputs": [],
   "source": [
    "class NeuralNetlayer:\n",
    "  def __init__(self):\n",
    "    self.gradient = None\n",
    "    self.parameters = None\n",
    "\n",
    "  def forward(self, x):\n",
    "    raise NotImplementedError\n",
    "\n",
    "  def backward(self, gradient):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "_SKkpiaXXmYl"
   },
   "outputs": [],
   "source": [
    "class LinearLayer(NeuralNetlayer):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super().__init__()\n",
    "    self.ni = input_size\n",
    "    self.no = output_size\n",
    "    # For a layer with ReLU activation\n",
    "    # He initialization\n",
    "    std = np.sqrt(2. / input_size)\n",
    "    self.w = np.random.randn(output_size, input_size) * std\n",
    "    self.b = np.random.randn(output_size)\n",
    "    self.cur_input = None\n",
    "    self.parameters = [self.w, self.b]\n",
    "\n",
    "  def forward(self, x):\n",
    "    self.cur_input = x\n",
    "    return x @ self.w.T + self.b\n",
    "\n",
    "  def backward(self, gradient):\n",
    "    assert self.cur_input is not None, \"Must call forward before backward!\"\n",
    "    dw = gradient.T @ self.cur_input\n",
    "    db = gradient.sum(axis=0)\n",
    "    self.gradient = [dw, db]\n",
    "    return gradient.dot(self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "Kv_AP7VvScrz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ReLULayer(NeuralNetlayer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.gradient = np.where(x > 0, 1.0, 0.0)\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        assert self.gradient is not None, \"Must call forward before backward\"\n",
    "        return gradient * self.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "gsBVKwhIqPPt"
   },
   "outputs": [],
   "source": [
    "class SoftmaxOutputLayer(NeuralNetlayer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cur_probs = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        exps = np.exp(x)\n",
    "        probs = exps / np.sum(exps, axis=-1)[:, None]\n",
    "        self.cur_probs = probs\n",
    "        return probs\n",
    "\n",
    "    def backward(self, target):\n",
    "        assert self.cur_probs is not None, \"Must call forward before backward\"\n",
    "        return self.cur_probs - target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "XVngHzZtqU7i"
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, *args: List[NeuralNetlayer]):\n",
    "        self.layers = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, target):\n",
    "        for layer in self.layers[::-1]:\n",
    "            target = layer.backward(target)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def fit(self, x, y, optimizer, epochs, lr):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(x)\n",
    "            loss = -np.sum(y * np.log(y_pred)) / len(y)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} Loss: {loss}\")\n",
    "            self.backward(y)\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "JWtqWoJqsC5D"
   },
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, net: MLP):\n",
    "        self.net = net\n",
    "\n",
    "    def step(self):\n",
    "        for layer in self.net.layers[::-1]:\n",
    "            if layer.parameters is not None:\n",
    "                self.update(layer.parameters, layer.gradient)\n",
    "\n",
    "    def update(self, params, gradient):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class GradientDescentOptimizer(Optimizer):\n",
    "    def __init__(self, net: MLP, lr: float, lamda: float = 0.0):\n",
    "        super().__init__(net)\n",
    "        self.lr = lr\n",
    "        self.lamda = lamda\n",
    "\n",
    "    # L2 regularization\n",
    "    def update(self, params, gradient):\n",
    "        w, b = params\n",
    "        dw, db = gradient\n",
    "        w -= self.lr * (dw + self.lamda * w)\n",
    "        b -= self.lr * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "JIulsf4_sKYm"
   },
   "outputs": [],
   "source": [
    "def train(mlp: MLP, optimizer: Optimizer, data_x, data_y, steps):\n",
    "    losses = []\n",
    "    labels = data_y\n",
    "    for _ in tqdm(range(steps)):\n",
    "        predictions = mlp.forward(data_x)\n",
    "        loss = -(labels * np.log(predictions)).sum(axis=-1).mean()\n",
    "        losses.append(loss)\n",
    "        mlp.backward(labels)\n",
    "        optimizer.step()\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Cross entropy loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sgd(mlp, optimizer, X_train, y_train, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle the training data\n",
    "        perm = np.random.permutation(X_train.shape[0])\n",
    "        X_train_shuffled = X_train[perm]\n",
    "        y_train_shuffled = y_train[perm]\n",
    "        \n",
    "        for i in range(X_train.shape[0]):\n",
    "            # Select a single training example\n",
    "            X_mini = X_train_shuffled[i:i+1]\n",
    "            y_mini = y_train_shuffled[i:i+1]\n",
    "            \n",
    "            # Perform a forward pass and compute the loss\n",
    "            predictions = mlp.forward(X_mini)\n",
    "            loss = -(y_mini * np.log(predictions)).sum()  # Example for cross-entropy loss\n",
    "            \n",
    "            # Backward pass to compute gradients\n",
    "            mlp.backward(y_mini)\n",
    "            \n",
    "            # Update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        # Optionally print the loss here to monitor training progress\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_acc(mlp: MLP, data_x, data_y):\n",
    "    predictions = mlp.forward(data_x)\n",
    "    return np.mean(np.argmax(predictions, axis=-1) == np.argmax(data_y, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "dCxRLqgusQlE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 Loss: 5.643200090234787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200 Loss: nan\n",
      "Epoch 3/200 Loss: nan\n",
      "Epoch 4/200 Loss: nan\n",
      "Epoch 5/200 Loss: nan\n",
      "Epoch 6/200 Loss: nan\n",
      "Epoch 7/200 Loss: nan\n",
      "Epoch 8/200 Loss: nan\n",
      "Epoch 9/200 Loss: nan\n",
      "Epoch 10/200 Loss: nan\n",
      "Epoch 11/200 Loss: nan\n",
      "Epoch 12/200 Loss: nan\n",
      "Epoch 13/200 Loss: nan\n",
      "Epoch 14/200 Loss: nan\n",
      "Epoch 15/200 Loss: nan\n",
      "Epoch 16/200 Loss: nan\n",
      "Epoch 17/200 Loss: nan\n",
      "Epoch 18/200 Loss: nan\n",
      "Epoch 19/200 Loss: nan\n",
      "Epoch 20/200 Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m opt2 \u001b[38;5;241m=\u001b[39m GradientDescentOptimizer(mlp2, \u001b[38;5;241m1e-3\u001b[39m, lamda\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# train(mlp2, opt2, X_train, y_train, GRADIENT_STEPS)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mmlp2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGRADIENT_STEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining accuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m, evaluate_acc(mlp2, X_test, y_test))\n",
      "Cell \u001b[0;32mIn[56], line 22\u001b[0m, in \u001b[0;36mMLP.fit\u001b[0;34m(self, x, y, optimizer, epochs, lr)\u001b[0m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(y \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(y_pred)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[56], line 12\u001b[0m, in \u001b[0;36mMLP.backward\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, target):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 12\u001b[0m         target \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[53], line 21\u001b[0m, in \u001b[0;36mLinearLayer.backward\u001b[0;34m(self, gradient)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust call forward before backward!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m dw \u001b[38;5;241m=\u001b[39m gradient\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_input\n\u001b[0;32m---> 21\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[43mgradient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient \u001b[38;5;241m=\u001b[39m [dw, db]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gradient\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/numpy/core/_methods.py:47\u001b[0m, in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     44\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_minimum(a, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out, keepdims, initial, where)\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prod\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     52\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_features = X_train.shape[-1]\n",
    "HIDDEN_SIZE = 64\n",
    "OUTPUT_SIZE = 24\n",
    "GRADIENT_STEPS = 200\n",
    "\n",
    "mlp2 = MLP(\n",
    "    LinearLayer(n_features, HIDDEN_SIZE),\n",
    "    ReLULayer(),\n",
    "    LinearLayer(HIDDEN_SIZE, HIDDEN_SIZE),\n",
    "    ReLULayer(),\n",
    "    LinearLayer(HIDDEN_SIZE, OUTPUT_SIZE),\n",
    "    SoftmaxOutputLayer()\n",
    ")\n",
    "opt2 = GradientDescentOptimizer(mlp2, 1e-3, lamda=1e-2)\n",
    "\n",
    "# train(mlp2, opt2, X_train, y_train, GRADIENT_STEPS)\n",
    "mlp2.fit(X_train, y_train, opt2, GRADIENT_STEPS, 1e-2)\n",
    "print(\"Training accuracy: \", evaluate_acc(mlp2, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test accuracy:\", evaluate_acc(mlp2, X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
